{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is was created in order to prepare files and calculate TFIDF scores for data\n",
    "located in file \"flat_data.txt\".\n",
    "During running this code following files are generated:\n",
    "+ vocabulary.txt\n",
    "+ inverted_indx.txt\n",
    "+ tfid.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import ItalianStemmer\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    ''' Function used for preprocessing text inside descriptions'''\n",
    "    text = text.lower()\n",
    "    # removing '\\n'\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    # removing punctuation\n",
    "    tokenizer = regexp_tokenize(text, \"[\\w\\$]+\")\n",
    "    # removing numbers\n",
    "    filtered = [w for w in tokenizer if not w.isnumeric()]\n",
    "    # filter the non stopwords\n",
    "    filtered = [w for w in filtered if not w in stopwords.words('italian')]\n",
    "    its = ItalianStemmer()\n",
    "    # removing the stem\n",
    "    filtered = [its.stem(word) for word in filtered]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING AND CREATING VOCABULARY FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set = set()\n",
    "annouc_list = []\n",
    "occurence_words_list = []\n",
    "\n",
    "# open file with our data\n",
    "with open(\"flat_data.txt\", \"r\" ,encoding=\"utf-8\") as flat_data:\n",
    "    reader = csv.reader(flat_data, delimiter=\",\")\n",
    "    for i, line in enumerate(reader):\n",
    "        #if i%100==0: print(i) #to see the progress of calculations\n",
    "        if line != [] and i!=0:\n",
    "            # preprocess the dictionary text\n",
    "            description = preprocess(line[5])\n",
    "            # put new words to vocabulary set\n",
    "            vocabulary_set.update(description)\n",
    "            # put prepared words into the list with all announcements\n",
    "            annouc_list.append(set(description))\n",
    "            # count words frequency\n",
    "            freq_word_dict = {}\n",
    "            for w in description:\n",
    "                try: freq_word_dict[w] += 1\n",
    "                except: freq_word_dict[w] = 1\n",
    "            # save the frequency dict for words in description\n",
    "            occurence_words_list.append(freq_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all collected words into \"vocabulary.txt\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {k:v for v, k in enumerate(vocabulary_set)}\n",
    "voc_file = open(\"vocabulary.txt\", 'w', encoding = \"utf8\")\n",
    "for term in vocabulary:\n",
    "    voc_file.write('{0}\\t{1}\\n'.format(term, vocabulary[term]))\n",
    "voc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING INVERTED INDEX\n",
    "Iterate through the annoucements:\n",
    "-> for each word inside the announcement create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted index dictionary has a structure: {id_word: announcements}\n",
    "inv_indx = defaultdict(set)\n",
    "for idx, words in enumerate(annouc_list):\n",
    "    for word in words:\n",
    "        inv_indx[vocabulary[word]].add(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to file - inverted_indx.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_file = open(\"inverted_indx.txt\", 'w', encoding = \"utf8\")\n",
    "for id_word, docks in inv_indx.items():\n",
    "    inv_file.write('{0}\\t{1}\\n'.format(id_word, '\\t'.join(map(str, docks))))\n",
    "inv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **computeTFID** calculates TFID score for a specific annoucement (doc_id) given as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFID(freq_dict, doc_id, tot_num_docs, inv_indx):\n",
    "    # freq_dict - dictionary with the frequency of words in the description\n",
    "    # doc_id - announcement_id\n",
    "    # inv_indx - inverted index (also saved in inverted_indx.txt)\n",
    "    # tot_num_docs - total number of annoucements >10000\n",
    "    tfid_per_annoucement = {}\n",
    "    for word in freq_dict.keys():\n",
    "        num_in_annouc = len(inv_indx[vocabulary[word]])\n",
    "        log_part = np.log(float(tot_num_docs)/num_in_annouc)\n",
    "        tfid_per_annoucement[word_id] = round(float(freq_dict[word])/numWords * log_part, 5)\n",
    "    tfid[doc_id] = tfid_per_annoucement\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all tfidf scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global tfid #global variable\n",
    "tfid = {}\n",
    "inv_indx_tfid = {}\n",
    "len_rows = len(docs_list)\n",
    "numWords = len(freq_dict)\n",
    "# iterate through all announcemets\n",
    "for doc_id, freq_dict in enumerate(occurence_words_list): \n",
    "    computeTFID(freq_dict, doc_id, len_rows, inv_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving **tfid** into the **\"tfid.csv\"** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_file = open(\"tfid.csv\", 'w', encoding = \"utf8\")\n",
    "for id_annouc, words_jacc_dict in tfid.items():\n",
    "    tfid_file.write('{0}\\t{1}\\n'.format(id_annouc, '\\t'.join(map(str, words_jacc_dict.keys()))))\n",
    "tfid_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
